---
title: Analysis at Scale
narratives: null
tags:
- analysis
- scale
---
**Scenario:**

Maritza is working with a really large and complex dataset stored across
multiple locations.  She has a workflow that she ran on the dataset and
she wants to update it as new data is added. She doesn’t have the memory or
compute power to do this as a robust process, and the data is too
large for her to download.

**Current approach:**

She would have to scrape together her own tools and some open source software to
make this happen. It would be an involved process that is not automatic, and would
use quite a bit of time running sequential processes that she’d rather
spend on the interesting research.  She would also not be able to do this at the scale
she wanted because the data would be inherently messy. It would require time
on a large computing hub at her institution, if they have one.

**With Data Commons Phase 1:**

Standards for interoperability of tools and harmonization of data and
metadata will let Maritza run this in the cloud.  She can run the
same workflow on all of her data, and she can automate these processes saving
time, and helping prevent her from making errors.

**With Data Commons longer vision:**

Maritza can access large datasets through single sign-on. The Data Commons
provides a collaborative environment for research and development so instead
of just using her idiosyncratic method, she can try validated workflows
created by others. This will help her ensure that results are robust
and reproducible.  She can also try analyses that are outside
her abiity to program herself.